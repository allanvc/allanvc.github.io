---
title: "Scraping Google News with 'rvest'"
author: "Allan Quadros"
date: 2018-08-21T21:13:14-05:00
categories: ["R"]
tags: ["rvest", "web scraping", "GoogleNews"]
---

## Scraping Google News with `rvest` (2020's update)

---

This is an example of how to scrape the Google News website with the `rvest` package. 

This post is a solution for a question from a WhatsApp \R group, [**blackbeltR**](https://www.blackbeltR.com.br/blog){target="_blank"}. A great deal of the basic ideas comes from his own code. I just kept it and added few things in order to get the code working. 

First off, you should take a look at the Google News website [HERE](https://news.google.com/){target="_blank"}, which I reproduce below:

<!-- ![](/post/web-scrap/![](/web-scrap/2018-08-21-google_news_scraping_files/google_news_screenshot.png)2018-08-21-google_news_scraping_files/google_news_screenshot.png) -->

![](img/screenshot1.png){width=100%}

You may notice, on the right side of the page, that we are using Google Chrome **dev-tools**. This is necessary in order to identify the *html nodes* we need. You can access this tool by hitting the **F12** key. The html nodes are passed as arguments to the `rvest` functions.

Basically, the idea is to extract the communication vehicle (vehicle), the time elapsed since the news was published (time), and the main headline (headline).

The code and comments are presented below: 

```{r, message=FALSE}
# loading the packages:
library(dplyr) # for pipes and the data_frame function
library(rvest) # webscraping
library(stringr) # to deal with strings and to clean up our data
```


```{r}
# extracting the whole website
google <- read_html("https://news.google.com/")
```


As we can see, he Google News website is divided in rectangular chunks of headlines and other info. Therefore, our strategy is to first scrape the whole chunks, and then, for each chunk scrape the information of interest: vehicle, time, and the headlines.

We start scraping the whole chunks of articles. By using the "inspect" tool of our browser, we can see that the `"article"` \HTML node is the one that identifies each chunk.

![](img/screenshot2.png){width=100%}


```{r}
# extracting the headlines
# and using stringr for cleaning
article_all <- google %>% html_nodes("article")

article_all

```

Having the whole chunks, now we can separately scrape the information of interest:

```{r}
times <- article_all %>%
  html_node("time") %>%
  html_text()

vehicles <- article_all %>%
  html_nodes("a.wEwyrc.AVN2gc.uQIVzc.Sksgp") %>%
  html_text()

headlines <- article_all %>%
  html_nodes("a.DY5T1d") %>%
  html_text()
```

Let's take a look on these vectors:

```{r}
# take a look at the first ten
headlines[1:10]

vehicles[1:10]

times[1:10]
```


It seems all good!

Then, we can proceed to generate our final `tibble`:

```{r}

tb_news <- tibble(headlines, vehicles, times)

tb_news
```

That's all!
